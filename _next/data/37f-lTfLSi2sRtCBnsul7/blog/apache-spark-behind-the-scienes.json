{"pageProps":{"post":{"mdxSource":"var Component=(()=>{var s=Object.create;var r=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var g=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,m=Object.prototype.hasOwnProperty;var k=(i,n)=>()=>(n||i((n={exports:{}}).exports,n),n.exports),y=(i,n)=>{for(var c in n)r(i,c,{get:n[c],enumerable:!0})},l=(i,n,c,h)=>{if(n&&typeof n==\"object\"||typeof n==\"function\")for(let a of g(n))!m.call(i,a)&&a!==c&&r(i,a,{get:()=>n[a],enumerable:!(h=p(n,a))||h.enumerable});return i};var v=(i,n,c)=>(c=i!=null?s(u(i)):{},l(n||!i||!i.__esModule?r(c,\"default\",{value:i,enumerable:!0}):c,i)),S=i=>l(r({},\"__esModule\",{value:!0}),i);var o=k((z,t)=>{t.exports=_jsx_runtime});var x={};y(x,{default:()=>D,frontmatter:()=>b});var e=v(o()),b={title:\"Apache Spark behind the scenes\",authors:[\"hung\"],date:\"2022-07-12\",tags:[\"engineering\",\"data\"],summary:\"C\\xE1c b\\u1EA1n c\\xF3 th\\u1EAFc m\\u1EAFc sau khi submit 1 job cho Spark Cluster th\\xEC Spark s\\u1EBD l\\xE0m nh\\u1EEFng g\\xEC kh\\xF4ng? C\\xF9ng t\\xECm hi\\u1EC3u v\\u1EDBi m\\xECnh nh\\xE9.\",layout:\"PostLayout\"};function d(i){let n=Object.assign({ul:\"ul\",li:\"li\",a:\"a\",h1:\"h1\",p:\"p\",img:\"img\",ol:\"ol\",h2:\"h2\",strong:\"strong\",h3:\"h3\",code:\"code\",h4:\"h4\",pre:\"pre\"},i.components);return(0,e.jsxs)(e.Fragment,{children:[(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsx)(n.li,{children:(0,e.jsx)(n.a,{href:\"#overview\",children:\"Overview\"})}),`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.a,{href:\"#spark-architecture\",children:\"Spark Architecture\"}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsx)(n.li,{children:(0,e.jsx)(n.a,{href:\"#cluster-manager\",children:\"Cluster Manager\"})}),`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.a,{href:\"#driver\",children:\"Driver\"}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsx)(n.li,{children:(0,e.jsx)(n.a,{href:\"#sparkcontext\",children:\"SparkContext\"})}),`\n`,(0,e.jsx)(n.li,{children:(0,e.jsx)(n.a,{href:\"#logical-execution-plan\",children:\"Logical execution plan\"})}),`\n`,(0,e.jsx)(n.li,{children:(0,e.jsx)(n.a,{href:\"#dagscheduler\",children:\"DagScheduler\"})}),`\n`,(0,e.jsx)(n.li,{children:(0,e.jsx)(n.a,{href:\"#taskscheduler\",children:\"TaskScheduler\"})}),`\n`,(0,e.jsx)(n.li,{children:(0,e.jsx)(n.a,{href:\"#backendscheduler\",children:\"BackendScheduler\"})}),`\n`,(0,e.jsx)(n.li,{children:(0,e.jsx)(n.a,{href:\"#blockmanager\",children:\"BlockManager\"})}),`\n`]}),`\n`]}),`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.a,{href:\"#catalyst-optimizer\",children:\"Catalyst Optimizer\"}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.a,{href:\"#logical-plan\",children:\"Logical Plan\"}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsx)(n.li,{children:(0,e.jsx)(n.a,{href:\"#unresolved-logical-plan\",children:\"Unresolved Logical Plan\"})}),`\n`,(0,e.jsx)(n.li,{children:(0,e.jsx)(n.a,{href:\"#analyzed-logical-plan\",children:\"Analyzed Logical Plan\"})}),`\n`,(0,e.jsx)(n.li,{children:(0,e.jsx)(n.a,{href:\"#optimize-logical-plan\",children:\"Optimize Logical Plan\"})}),`\n`]}),`\n`]}),`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.a,{href:\"#physical-plan\",children:\"Physical plan\"}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsx)(n.li,{children:(0,e.jsx)(n.a,{href:\"#operator\",children:\"Operator\"})}),`\n`,(0,e.jsx)(n.li,{children:(0,e.jsx)(n.a,{href:\"#additional-rules\",children:\"Additional Rules\"})}),`\n`]}),`\n`]}),`\n`]}),`\n`]}),`\n`,(0,e.jsx)(n.li,{children:(0,e.jsx)(n.a,{href:\"#executor\",children:\"Executor\"})}),`\n`]}),`\n`]}),`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.a,{href:\"#references\",children:\"References\"}),`\n`,(0,e.jsx)(\"br\",{}),`\n`,(0,e.jsx)(\"br\",{}),`\n`]}),`\n`]}),`\n`,(0,e.jsx)(n.h1,{children:\"Overview\"}),`\n`,(0,e.jsx)(n.p,{children:\"C\\xF3 bao gi\\u1EDD b\\u1EA1n th\\u1EAFc m\\u1EAFc, sau khi b\\u1EA1n submit m\\u1ED9t app v\\xE0o Spark cluster \\u0111\\u1EC3 th\\u1EF1c hi\\u1EC7n nh\\u1EEFng t\\xEDnh to\\xE1n ho\\u1EB7c bi\\u1EBFn \\u0111\\u1ED5i tr\\xEAn d\\u1EEF li\\u1EC7u, th\\xEC Spark s\\u1EBD l\\xE0m g\\xEC kh\\xF4ng?\"}),`\n`,(0,e.jsx)(n.p,{children:\"B\\xE0i vi\\u1EBFt n\\xE0y m\\xECnh s\\u1EBD chia s\\u1EBB t\\u1ED5ng quan Spark ho\\u1EA1t \\u0111\\u1ED9ng nh\\u01B0 th\\u1EBF n\\xE0o.\"}),`\n`,(0,e.jsx)(n.h1,{children:\"Spark Architecture\"}),`\n`,(0,e.jsx)(n.p,{children:\"Spark Architecture g\\u1ED3m nh\\u1EEFng g\\xEC?\"}),`\n`,(0,e.jsx)(n.p,{children:(0,e.jsx)(n.img,{src:\"/media/2022/07/spark-behind-1.png\",alt:\"\"})}),`\n`,(0,e.jsx)(n.p,{children:\"Quan s\\xE1t h\\xECnh ta th\\u1EA5y Spark c\\xF3 3 th\\xE0nh ph\\u1EA7n ch\\xEDnh:\"}),`\n`,(0,e.jsxs)(n.ol,{children:[`\n`,(0,e.jsx)(n.li,{children:\"Cluster Manager\"}),`\n`,(0,e.jsx)(n.li,{children:\"Driver\"}),`\n`,(0,e.jsx)(n.li,{children:\"Worker\"}),`\n`]}),`\n`,(0,e.jsx)(n.p,{children:\"Th\\xF4ng th\\u01B0\\u1EDDng Spark s\\u1EBD ho\\u1EA1t \\u0111\\u1ED9ng v\\u1EDBi master node v\\xE0 nhi\\u1EC1u worker node, gi\\u1ED1ng v\\u1EDBi Hadoop master and slave node.\"}),`\n`,(0,e.jsx)(n.p,{children:\"D\\u01B0\\u1EDBi \\u0111\\xE2y mi\\xEAu t\\u1EA3 chi ti\\u1EBFt m\\u1ED7i th\\xE0nh ph\\u1EA7n s\\u1EBD l\\xE0m nh\\u1EEFng g\\xEC.\"}),`\n`,(0,e.jsx)(n.h2,{children:\"Cluster Manager\"}),`\n`,(0,e.jsx)(n.p,{children:\"Cluster manager l\\xE0 m\\u1ED9t platform m\\xE0 ch\\xFAng ta deploy ho\\u1EB7c ch\\u1EA1y Spark tr\\xEAn \\u0111\\xF3 l\\u01B0u \\xFD l\\xE0 ch\\u1EC9 v\\u1EDBi cluster mode (local mode m\\xECnh s\\u1EBD kh\\xF4ng \\u0111\\u1EC1 c\\u1EADp \\u1EDF \\u0111\\xE2y)\"}),`\n`,(0,e.jsx)(n.p,{children:\"C\\xF3 nh\\u1EEFng ki\\u1EC3u cluster manager nh\\u01B0 sau:\"}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsx)(n.li,{children:\"Kubernetes: Hi\\u1EC7n t\\u1EA1i, Spark \\u1EDF Fossil \\u0111\\u01B0\\u1EE3c deploy tr\\xEAn Kubernetes (k8s m\\u1ED9t h\\u1EC7 th\\u1ED1ng open source d\\xF9ng \\u0111\\u1EC3 t\\u1EF1 \\u0111\\u1ED9ng deploy, qu\\u1EA3n l\\xFD, m\\u1EDF r\\u1ED9ng cho container)\"}),`\n`,(0,e.jsx)(n.li,{children:\"Hadoop Yarn: H\\u1EC7 th\\u1ED1ng qu\\u1EA3n l\\xFD Big data Hadoop\"}),`\n`,(0,e.jsx)(n.li,{children:\"Standalone: \\u0110\\xE2y l\\xE0 ki\\u1EC3u \\u0111\\u01B0\\u1EE3c Spark h\\u1ED7 tr\\u1EE3 \\u0111\\u1EC3 t\\u1EF1 d\\u1EF1ng 1 cluster c\\u01A1 b\\u1EA3n m\\u1ED9t c\\xE1ch d\\u1EC5 d\\xE0ng\"}),`\n`,(0,e.jsx)(n.li,{children:\"Apache Mesos (Deprecated)\"}),`\n`]}),`\n`,(0,e.jsx)(n.h2,{children:\"Driver\"}),`\n`,(0,e.jsx)(n.p,{children:(0,e.jsx)(n.img,{src:\"/media/2022/07/spark-behind-2.png\",alt:\"\"})}),`\n`,(0,e.jsx)(n.p,{children:\"Spark driver l\\xE0 th\\xE0nh ph\\u1EA7n ch\\xEDnh cho nh\\u1EEFng Spark app, n\\xF3 th\\u01B0\\u1EDDng n\\u1EB1m \\u1EDF master node (Standalone mode, Hadoop Yarn). Tuy nhi\\xEAn \\u1EDF Fossil Spark \\u0111\\u01B0\\u1EE3c deploy tr\\xEAn k8s n\\xEAn m\\u1ED7i l\\u1EA7n client submit m\\u1ED9t spark app v\\xE0o k8s cluster, k8s s\\u1EBD t\\u1EA1o n\\xEAn m\\u1ED9t pod driver cho 1 spark app.\"}),`\n`,(0,e.jsxs)(n.p,{children:[\"Sau khi Driver \\u0111\\u01B0\\u1EE3c t\\u1EA1o ra, th\\xEC n\\xF3 s\\u1EBD chuy\\u1EC3n \\u0111\\u1ED5i user application th\\xE0nh t\\u1EEBng ph\\u1EA7n nh\\u1ECF \\u0111\\u01B0\\u1EE3c g\\u1ECDi l\\xE0 Job v\\xE0 task. Sau \\u0111\\xF3 s\\u1EBD ch\\u1ECBu tr\\xE1ch nhi\\u1EC7m giao ti\\u1EBFp v\\u1EDBi qu\\u1EA3n l\\xFD t\\xE0i nguy\\xEAn \\u0111\\u1EC3 y\\xEAu c\\u1EA7u t\\xE0i nguy\\xEAn (executor) v\\xE0 ph\\xE2n b\\u1ED5 t\\xE0i nguy\\xEAn cho c\\xE1c executor, Driver s\\u1EBD qu\\xE9t qua Spark app code \\u0111\\u1EC3 bi\\u1EBFt \\u0111\\u01B0\\u1EE3c \\u0111\\xE2u l\\xE0 \",(0,e.jsx)(n.strong,{children:\"transformation\"}),\" \\u0111\\xE2u l\\xE0 \",(0,e.jsx)(n.strong,{children:\"action\"}),\" \\u0111\\u1EC3 t\\u1EA1o ra Spark Execution plan (Logical plan v\\xE0 Physical plan). Ngo\\xE0i vi\\u1EC7c ph\\xE2n b\\u1ED5 t\\xE0i nguy\\xEAn v\\xE0 l\\xEAn l\\u1ECBch Job cho Executor th\\u1EF1c thi, Driver c\\xF2n l\\xE0m nh\\u1EEFng nhi\\u1EC7m v\\u1EE5 kh\\xE1c nh\\u01B0: Collect status c\\u1EE7a t\\u1EEBng executor, Collect nh\\u1EEFng executor metrics v\\xE0 hi\\u1EC3n th\\u1ECB l\\xEAn Spark UI.\"]}),`\n`,(0,e.jsx)(n.p,{children:\"Driver c\\xF3 r\\u1EA5t nhi\\u1EC1u th\\xE0nh ph\\u1EA7n.M\\u1ED9t s\\u1ED1 th\\xE0nh ph\\u1EA7n ch\\xEDnh bao g\\u1ED3m:\"}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsx)(n.li,{children:\"SparkContext\"}),`\n`,(0,e.jsx)(n.li,{children:\"Logical execution plan\"}),`\n`,(0,e.jsx)(n.li,{children:\"DagScheduler\"}),`\n`,(0,e.jsx)(n.li,{children:\"TaskScheduler\"}),`\n`,(0,e.jsx)(n.li,{children:\"BackendScheduler\"}),`\n`,(0,e.jsx)(n.li,{children:\"BlockManager\"}),`\n`]}),`\n`,(0,e.jsx)(n.p,{children:\"\\u0110\\xE2y l\\xE0 flow x\\u1EED l\\xFD RDD\"}),`\n`,(0,e.jsx)(n.p,{children:\"C\\xF2n v\\u1EDBi DataFrame ho\\u1EB7c SQL ho\\u1EB7c DataSet ta s\\u1EBD c\\xF3 1 flow kh\\xE1c s\\u1EED d\\u1EE5ng Catalyst Optimizer\"}),`\n`,(0,e.jsx)(n.p,{children:\"2 flow ch\\u1EC9 kh\\xE1c nhau \\u1EDF ch\\u1ED7 Logical plan v\\xE0 physical plan.\"}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsx)(n.li,{children:\"V\\u1EDBi RDD Logical Execution plan s\\u1EBD kh\\xF4ng \\u0111i qua Catalyst Optimize m\\xE0 s\\u1EBD \\u0111i th\\u1EB3ng v\\xE0o DagScheduler \\u0111\\u1EC3 th\\u1EF1c thi.\"}),`\n`,(0,e.jsx)(n.li,{children:\"Ri\\xEAng v\\u1EDBi DataFrame, SQL ho\\u1EB7c DataSet s\\u1EBD \\u0111i qua Catalyst Optimize tr\\u01B0\\u1EDBc khi v\\xE0o DagScheduler.\"}),`\n`]}),`\n`,(0,e.jsx)(n.h3,{children:\"SparkContext\"}),`\n`,(0,e.jsx)(n.p,{children:\"Spark Context l\\xE0 main entrypoint c\\u1EE7a t\\u1EA5t c\\u1EA3 c\\xE1c th\\xE0nh ph\\u1EA7n trong Spark, th\\xE0nh ph\\u1EA7n ch\\xEDnh c\\u1EE7a t\\u1EA5t c\\u1EA3 Spark App.\"}),`\n`,(0,e.jsx)(n.h3,{children:\"Logical execution plan\"}),`\n`,(0,e.jsx)(n.p,{children:\"Logical execution plan l\\xE0 1 abstract c\\u1EE7a c\\xE1c b\\u01B0\\u1EDBc chuy\\u1EC3n \\u0111\\u1ED5i c\\u1EA7n \\u0111\\u01B0\\u1EE3c th\\u1EF1c hi\\u1EC7n (C\\xF3 bao nhi\\xEAu RDDs s\\u1EBD \\u0111\\u01B0\\u1EE3c t\\u1EA1o ra, b\\u1EDFi v\\xEC RDD imutable n\\xEAn m\\u1ED7i l\\u1EA7n transform s\\u1EBD t\\u1EA1o ra 1 ho\\u1EB7c nhi\\u1EC1u RDD m\\u1EDBi t\\xF9y v\\xE0o Narrow ho\\u1EB7c Wide transform) c\\xF2n \\u0111\\u01B0\\u1EE3c g\\u1ECDi l\\xE0 RDD lineage. Nh\\u01B0 v\\u1EADy Logical execution plan \\u0111\\u01B0\\u1EE3c t\\u1EA1o b\\u1EDFi t\\u1EEBng transformation c\\u1EE7a t\\u1EEBng RDD v\\xE0 l\\u01B0u \\u1EDF SparkContext.\"}),`\n`,(0,e.jsx)(n.h3,{children:\"DagScheduler\"}),`\n`,(0,e.jsxs)(n.p,{children:[\"Sau khi Logical execution plan \\u0111\\xE3 \\u0111\\u01B0\\u1EE3c t\\u1EA1o, khi c\\xF3 1 action \\u0111\\u01B0\\u1EE3c g\\u1ECDi DagScheduler s\\u1EBD chuy\\u1EC3n n\\xF3 th\\xE0nh Physical execution plan (b\\u1EB1ng c\\xE1ch s\\u1EED d\\u1EE5ng \",(0,e.jsx)(n.strong,{children:\"Job\"}),\" v\\xE0 \",(0,e.jsx)(n.strong,{children:\"Stage\"}),\"). \\u0110\\u1EC3 ph\\xE2n bi\\u1EC7t \\u0111\\u01B0\\u1EE3c \\u0111\\xE2u l\\xE0 method action \\u0111\\xE2u l\\xE0 method transform m\\xECnh \\u0111\\xE3 ki\\u1EBFm \\u0111\\u01B0\\u1EE3c 1 slide c\\xF3 nh\\u1EEFng th\\xF4ng tin n\\xE0y c\\u1EA3m \\u01A1n v\\xE0o t\\xE1c gi\\u1EA3 c\\xF3 t\\xEAn l\\xE0 Jeff Thomspon \",(0,e.jsx)(n.a,{href:\"https://training.databricks.com/visualapi.pdf\",children:\"link\"}),\".\"]}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsx)(n.li,{children:\"Job: M\\u1ED7i m\\u1ED9t job l\\xE0 1 action trong Spark. Action s\\u1EBD th\\u1EF1c hi\\u1EC7n job tr\\xEAn cluster v\\xE0 return value l\\u1EA1i cho Spark Driver.\"}),`\n`,(0,e.jsxs)(n.li,{children:[\"Stage: M\\u1ED7i 1 job s\\u1EBD c\\xF3 nhi\\u1EC1u stage. S\\u1ED1 l\\u01B0\\u1EE3ng stage s\\u1EBD ph\\u1EE5 thu\\u1ED9c v\\xE0o b\\u1EA1n th\\u1EF1c hi\\u1EC7n Narrow Transformation ho\\u1EB7c Wide Transformation. T\\u1EA5t c\\u1EA3 Narrow Transformation (map, flatmap, \\u2026) s\\u1EBD \\u0111\\u01B0\\u1EE3c th\\u1EF1c hi\\u1EC7n tr\\xEAn 1 stage. Kh\\xE1c v\\u1EDBi Narrow, Wide Transformation s\\u1EBD t\\u1EA1o ra 1 stage m\\u1EDBi \\u0111i\\u1EC1u n\\xE0y d\\u1EABn t\\u1EDBi m\\u1ED7i 1 stage s\\u1EBD c\\xF3 \",(0,e.jsx)(n.strong,{children:\"stage boundary\"}),\". M\\u1ED7i 1 Stage Spark s\\u1EBD l\\u01B0u data \\u1EDF local disk.\"]}),`\n`,(0,e.jsx)(n.li,{children:\"Task: M\\u1ED7i 1 Stage s\\u1EBD c\\xF3 nhi\\u1EC1u task, m\\u1ED7i 1 task s\\u1EBD \\u1EE9ng v\\u1EDBi 1 partition.\"}),`\n`]}),`\n`,(0,e.jsx)(n.p,{children:\"Nhi\\u1EC7m v\\u1EE5 c\\u1EE7a DagScheduler:\"}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsxs)(n.li,{children:[\"S\\u1EBD t\\xEDnh to\\xE1n v\\xE0 t\\u1EA1o ra exection DAG (DAG c\\u1EE7a nh\\u1EEFng stages) cho 1 job sau \\u0111\\xF3 submits nh\\u1EEFng stage \\u0111\\xF3 cho \",(0,e.jsx)(n.strong,{children:\"TaskScheduler.\"})]}),`\n`,(0,e.jsx)(n.li,{children:\"X\\xE1c \\u0111\\u1ECBnh preferred locations (V\\u1ECB tr\\xED host, executor id) \\u0111\\u1EC3 run task, ngo\\xE0i ra n\\xF3 n\\xF3 tracking RDD n\\xE0o \\u0111\\xE3 \\u0111\\u01B0\\u1EE3c cache \\u0111\\u1EC3 kh\\xF4ng recompute l\\u1EA1i.\"}),`\n`,(0,e.jsx)(n.li,{children:\"X\\u1EED l\\xFD failures, resumitted nguy\\xEAn 1 stage n\\u1EBFu c\\xF3 1 task n\\xE0o \\u0111\\xF3 b\\u1ECB l\\u1ED7i.\"}),`\n`]}),`\n`,(0,e.jsx)(n.p,{children:\"DagScheduler s\\u1EED d\\u1EE5ng event driven architect, N\\u1EBFu nh\\u01B0 c\\xF3 1 job m\\u1EDBi \\u0111\\u01B0\\u1EE3c submit th\\xEC DagScheduler s\\u1EBD \\u0111\\u1ECDc v\\xE0 th\\u1EF1c hi\\u1EC7n 1 c\\xE1ch tu\\u1EA7n t\\u1EF1.\"}),`\n`,(0,e.jsx)(n.h3,{children:\"TaskScheduler\"}),`\n`,(0,e.jsxs)(n.p,{children:[\"TaskScheduler s\\u1EBD nh\\u1EADn set of task \\u0111\\xE3 \\u0111\\u01B0\\u1EE3c submit b\\u1EDFi \",(0,e.jsx)(n.strong,{children:\"DagScheduler\"}),\" cho t\\u1EEBng stage, v\\xE0 c\\xF3 nhi\\u1EC7m v\\u1EE5 schedule v\\xE0 sending task cho worker ho\\u1EB7c executor th\\u1EF1c hi\\u1EC7n, retry n\\u1EBFu nh\\u01B0 b\\u1ECB l\\u1ED7i.\"]}),`\n`,(0,e.jsx)(n.h3,{children:\"BackendScheduler\"}),`\n`,(0,e.jsx)(n.p,{children:\"BackendScheduler s\\u1EBD h\\u1ED7 tr\\u1EE3 nhi\\u1EC1u lo\\u1EA1i cluster manager nh\\u01B0: Hadoop-Yarn, Kubernetes, Apache Mesos.\"}),`\n`,(0,e.jsxs)(n.p,{children:[\"Khi Spark app y\\xEAu c\\u1EA7u resource t\\u1EEB cluster manager \\u0111\\u1EC3 th\\u1EF1c thi, n\\u1EBFu nh\\u01B0 \",(0,e.jsx)(n.strong,{children:\"BackendScheduler\"}),\" nh\\u1EADn \\u0111\\u01B0\\u1EE3c resource allocate b\\u1EDFi cluster manager, n\\xF3 c\\xF3 th\\u1EC3 start executor.\"]}),`\n`,(0,e.jsx)(n.h3,{children:\"BlockManager\"}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"BlockManager\"}),\" l\\xE0 n\\u01A1i l\\u01B0u tr\\u1EEF block of data d\\u01B0\\u1EDBi d\\u1EA1ng key-value v\\xE0 ch\\u1EA1y tr\\xEAn t\\u1EA5t c\\u1EA3 c\\xE1c node trong Spark App v\\xED d\\u1EE5 nh\\u01B0: Driver, Executor. N\\xF3 upload v\\xE0 fetch data block \\u1EDF local v\\xE0 remote s\\u1EED d\\u1EE5ng nhi\\u1EC1u ki\\u1EC3u l\\u01B0u tr\\u1EEF nh\\u01B0: memory, disk, off-heap.\"]}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsxs)(n.li,{children:[\"N\\u1EBFu nh\\u01B0 Result tr\\u1EA3 v\\u1EC1 qu\\xE1 l\\u1EDBn, n\\xF3 s\\u1EBD \\u0111\\u01B0\\u1EE3c persisted \\u1EDF \\u201Cmemory + disk\\u201D \\u0111\\u01B0\\u1EE3c qu\\u1EA3n l\\xFD b\\u1EDFi \",(0,e.jsx)(n.strong,{children:\"BlockManager\"}),\". Driver s\\u1EBD get result th\\xF4ng qua \",(0,e.jsx)(n.strong,{children:\"indirectResult (Storage location)\"}),\". Khi n\\xE0o c\\u1EA7n Driver s\\u1EBD fetch n\\xF3 qua HTTP.\"]}),`\n`,(0,e.jsxs)(n.li,{children:[\"N\\u1EBFu nh\\u01B0 Result tr\\u1EA3 v\\u1EC1 nh\\u1ECF h\\u01A1n 10mb (\",(0,e.jsx)(n.code,{children:\"spark.akka.frameSize = 10MB\"}),\"). N\\xF3 s\\u1EBD \\u0111\\u01B0\\u1EE3c g\\u1EEDi v\\u1EC1 th\\u1EB3ng driver th\\xF4ng qua \",(0,e.jsx)(n.strong,{children:\"directResult\"}),\".\"]}),`\n`]}),`\n`,(0,e.jsx)(n.h2,{children:\"Catalyst Optimizer\"}),`\n`,(0,e.jsx)(n.p,{children:(0,e.jsx)(n.img,{src:\"/media/2022/07/spark-behind-3.png\",alt:\"\"})}),`\n`,(0,e.jsxs)(n.p,{children:[\"M\\xECnh s\\u1EBD n\\xF3i s\\u01A1 Catalyst Optimizer l\\xE0 g\\xEC. Catalyst Optimizer l\\xE0 Core c\\u1EE7a SQL query v\\xE0 DataFrame, Catalyst Optimizer h\\u1ED7 tr\\u1EE3 \",(0,e.jsx)(n.strong,{children:\"Rules-based optimization\"}),\" (T\\u1EA5t c\\u1EA3 c\\xE1c Ruled \\u0111\\u1EC3 Optimize v\\xE0 Analysis) v\\xE0 \",(0,e.jsx)(n.strong,{children:\"Cost-base optimization\"}),\" (S\\u1EED d\\u1EE5ng c\\xE1c Rule c\\u1EE7a Rules-based \\u0111\\u1EC3 optimize d\\u1EF1a v\\xE0o th\\u1ED1ng k\\xEA v\\xE0 t\\xEDnh to\\xE1n). V\\u1EADy Catalyst s\\u1EBD l\\xE0m nh\\u01B0 th\\u1EBF n\\xE0o? Catalyst s\\u1EED d\\u1EE5ng c\\u1EA5u tr\\xFAc d\\u1EEF li\\u1EC7u c\\xE2y \\u0111\\u1EC3 x\\xE2y d\\u1EF1ng query plan ho\\u1EB7c x\\xE2y d\\u1EF1ng c\\xE2y c\\u1EE7a nh\\u1EEFng expression, c\\xE1c node c\\u1EE7a c\\xE2y \\u0111\\u01B0\\u1EE3c \\u0111\\u1ECBnh ngh\\u0129a b\\u1EB1ng Scala nh\\u01B0 l\\xE0 subclass c\\u1EE7a TreeNode class (V\\xED d\\u1EE5: 1 node c\\u1EE7a c\\xE2y c\\xF3 th\\u1EC3 l\\xE0 datatype d\\u1EA1ng int, ho\\u1EB7c function add c\\u1ED9ng 2 s\\u1ED1 int). Catalyst s\\u1EED d\\u1EE5ng Rules optimize \\u0111\\u01B0\\u1EE3c \\u0111\\u1ECBnh ngh\\u0129a s\\u1EB5n \\u0111\\u1EC3 bi\\u1EBFn \\u0111\\u1ED5i m\\u1ED9t c\\xE2y th\\xE0nh 1 c\\xE2y t\\u1ED1i \\u01B0u h\\u01A1n (l\\xE1t n\\u1EEFa m\\xECnh s\\u1EBD c\\xF3 v\\xED d\\u1EE5).\"]}),`\n`,(0,e.jsx)(n.h3,{children:\"Logical Plan\"}),`\n`,(0,e.jsx)(n.h4,{children:\"Unresolved Logical Plan\"}),`\n`,(0,e.jsxs)(n.p,{children:[\"Theo nh\\u01B0 m\\xECnh t\\xECm hi\\u1EC3u th\\xEC khi code Spark app c\\u1EE7a ch\\xFAng ta \\u0111\\xFAng syntax v\\xE0 valid nh\\u01B0ng t\\xEAn c\\u1EE7a c\\xE1c column v\\xE0 c\\xE1c b\\u1EA3ng trong query ho\\u1EB7c trong dataframe c\\u1EE7a c\\xE1c b\\u1EA1n b\\u1ECB sai ho\\u1EB7c kh\\xF4ng t\\u1ED3n t\\u1EA1i n\\u1EBFu \\u0111\\xFAng th\\xEC Spark s\\u1EBD raise l\\u1ED7i ngay v\\xE0o l\\xFAc n\\xE0y, nh\\u01B0ng Spark v\\u1EABn s\\u1EBD t\\u1EA1o ra m\\u1ED9t \",(0,e.jsx)(n.strong,{children:\"Unresolved Logic Plan/Parsed Logical Plan\"}),\" (Blank Logical plan).\"]}),`\n`,(0,e.jsx)(n.h4,{children:\"Analyzed Logical Plan\"}),`\n`,(0,e.jsxs)(n.p,{children:[\"Sau khi Spark t\\u1EA1o ra Unresolved Logical Plan s\\u1EBD \\u0111i qua componen Catalog. Catalog l\\xE0 n\\u01A1i ch\\u1EE9a c\\xE1c metadata c\\u1EE7a dataFrame, Spark table, Dataset. Spark s\\u1EBD s\\u1EED d\\u1EE5ng nh\\u1EEFng Rule \\u1EDF Catalyst v\\xE0 Catalog s\\u1EBD gi\\xFAp Spark check nh\\u1EEFng column name, data type \\u0111\\u1EC3 resolve v\\xE0 s\\u1EBD t\\u1EA1o ra \",(0,e.jsx)(n.strong,{children:\"Logical plan/Analyzed Logical plan\"}),\".\"]}),`\n`,(0,e.jsx)(n.h4,{children:\"Optimize Logical Plan\"}),`\n`,(0,e.jsx)(n.p,{children:\"Sau khi Logical plan \\u0111\\u01B0\\u1EE3c t\\u1EA1o ra s\\u1EBD qua 1 b\\u01B0\\u1EDBc l\\xE0 Logical Optimize, \\u1EDF b\\u01B0\\u1EDBc n\\xE0y Catalyst s\\u1EBD optimize l\\u1EA1i logical plan c\\u1EE7a ch\\xFAng ta.\"}),`\n`,(0,e.jsx)(n.p,{children:\"V\\xED d\\u1EE5: m\\xECnh c\\xF3 2 Dataframes:\"}),`\n`,(0,e.jsx)(n.p,{children:(0,e.jsx)(n.img,{src:\"/media/2022/07/spark-behind-4.png\",alt:\"\"})}),`\n`,(0,e.jsx)(n.p,{children:(0,e.jsx)(n.img,{src:\"/media/2022/07/spark-behind-5.png\",alt:\"\"})}),`\n`,(0,e.jsx)(n.p,{children:\"B\\xE2y gi\\u1EDD m\\xECnh s\\u1EBD th\\u1EF1c hi\\u1EC7n c\\xE1c b\\u01B0\\u1EDBc transform nh\\u01B0 sau:\"}),`\n`,(0,e.jsx)(n.pre,{children:(0,e.jsx)(n.code,{className:\"language-python\",children:`df3 = df1.join(df2, df1.dep_id == df2.dep_id, \"inner\")\n        .filter(df1.salary >= 4000)\n        .withColumn(\"salary\", df1.salary*3)\n        .filter((df1.firstname == \"Duyet\") | (df1.firstname == \"Duong\"))\n`})}),`\n`,(0,e.jsx)(n.p,{children:\"\\u0110\\u1EA7u ti\\xEAn m\\xECnh s\\u1EBD join Dataframe l\\u1EA1i v\\u1EDBi nhau. Filter nh\\u1EEFng ai c\\xF3 salary >= 4000, Sau \\u0111\\xF3 nh\\xE2n 3 gi\\xE1 tr\\u1ECB c\\u1EE7a c\\u1ED9t salary \\u1EDF df1, sau \\u0111\\xF3 filter firstname l\\xE0 Duyet ho\\u1EB7c Duong. \\u0110\\xE2y s\\u1EBD l\\xE0 expected bahavior m\\xE0 ch\\xFAng ta mu\\u1ED1n. B\\xE2y gi\\u1EDD ch\\xFAng ta s\\u1EBD xem Logical plan m\\xE0 Spark s\\u1EBD t\\u1EA1o ra nh\\xE9\"}),`\n`,(0,e.jsx)(n.pre,{children:(0,e.jsx)(n.code,{className:\"language-python\",children:`df3.explain(True)\n`})}),`\n`,(0,e.jsx)(n.p,{children:(0,e.jsx)(n.img,{src:\"/media/2022/07/spark-behind-6.png\",alt:\"\"})}),`\n`,(0,e.jsx)(n.p,{children:\"Nh\\u01B0 c\\xE1c b\\u1EA1n th\\u1EA5y Unresolved logical plan s\\u1EBD kh\\xF4ng hi\\u1EC3n th\\u1ECB c\\xE1c data type c\\u1EE7a data\"}),`\n`,(0,e.jsx)(n.p,{children:(0,e.jsx)(n.img,{src:\"/media/2022/07/spark-behind-7.png\",alt:\"\"})}),`\n`,(0,e.jsx)(n.p,{children:\"Sau khi Analyzed Spark s\\u1EBD bi\\u1EBFt \\u0111\\u01B0\\u1EE3c c\\xE1c data type c\\u1EE7a c\\xE1c column tr\\xEAn data.\"}),`\n`,(0,e.jsx)(n.p,{children:\"B\\xE2y gi\\u1EDD h\\xE3y \\u0111\\u1ECDc c\\xE1i plan n\\xE0y nha. \\u0110\\u1EC3 \\u0111\\u1ECDc plan c\\u1EE7a Spark ch\\xFAng ta s\\u1EBD \\u0111\\u1ECDc ng\\u01B0\\u1EE3c, \\u0111\\u1ECDc t\\u1EEB d\\u01B0\\u1EDBi l\\xEAn tr\\xEAn. \\u0110\\u1EA7u ti\\xEAn s\\u1EBD l\\xE0\"}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsx)(n.li,{children:\"Join 2 dataframe\"}),`\n`,(0,e.jsx)(n.li,{children:\"Filter Salary >= 4000\"}),`\n`,(0,e.jsx)(n.li,{children:\"Project (Select) c\\xE1c c\\u1ED9t \\u0111\\u1ED3ng th\\u1EDDi c\\u1ED9t salary * 3\"}),`\n`,(0,e.jsx)(n.li,{children:\"B\\u01B0\\u1EDBc cu\\u1ED1i c\\xF9ng s\\u1EBD filer l\\u1EA1i nh\\u1EEFng ng\\u01B0\\u1EDDi c\\xF3 t\\xEAn l\\xE0 Duyet ho\\u1EB7c Duong\"}),`\n`]}),`\n`,(0,e.jsx)(n.p,{children:\"R\\xF5 r\\xE0ng c\\xE1c b\\u01B0\\u1EDBc transform n\\xE0y kh\\xF4ng t\\u1ED1i \\u01B0u. N\\u1EBFu l\\xE0 m\\xECnh s\\u1EBD vi\\u1EBFt 1 c\\xE1ch t\\u1ED1i \\u01B0u h\\u01A1n b\\u1EB1ng c\\xE1ch filter tr\\u01B0\\u1EDBc nh\\u1EEFng \\u0111i\\u1EC1u ki\\u1EC7n c\\xF3 s\\u1EB5n r\\u1ED3i m\\u1EDBi join 2 data sau c\\xF9ng, nh\\u01B0ng \\u0111\\xE2y l\\xE0 v\\xED d\\u1EE5 \\u0111\\u1EC3 th\\u1EA5y \\u0111\\u01B0\\u1EE3c Spark Catalyst s\\u1EBD optimize nh\\u01B0 th\\u1EBF n\\xE0o. M\\xECnh ti\\u1EBFp t\\u1EE5c nh\\xECn xem Optimize Logical Plan sau khi Spark Optimize nh\\xE9.\"}),`\n`,(0,e.jsx)(n.p,{children:(0,e.jsx)(n.img,{src:\"/media/2022/07/spark-behind-8.png\",alt:\"\"})}),`\n`,(0,e.jsx)(n.p,{children:\"Sau khi Optimized ch\\xFAng ta th\\u1EA5y c\\xE1c step \\u0111\\xE3 \\u0111\\u01B0\\u1EE3c t\\u1EF1 \\u0111\\u1ED9ng thay \\u0111\\u1ED5i v\\xE0 \\u0111\\u01B0\\u1EE3c g\\u1ED9p l\\u1EA1i chung v\\u1EDBi nhau\"}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsx)(n.li,{children:\"\\u0110\\u1EA7u ti\\xEAn s\\u1EBD filter df2 c\\u1ED9t dep_id not null\"}),`\n`,(0,e.jsx)(n.li,{children:\"Ti\\u1EBFp theo s\\u1EBD g\\u1ED9p filter Salary >= 4000 v\\xE0 filter firstname v\\xE0 c\\u1ED9t dep_id not null cho df1\"}),`\n`,(0,e.jsx)(n.li,{children:\"Cu\\u1ED1i c\\xF9ng m\\u1EDBi join\"}),`\n`]}),`\n`,(0,e.jsx)(n.p,{children:\"R\\xF5 r\\xE0ng l\\xE0 \\u0111\\xE3 Optimize h\\u01A1n c\\xE1c b\\u01B0\\u1EDBc Expect behavior.\"}),`\n`,(0,e.jsx)(n.p,{children:\"Tuy nhi\\xEAn m\\xECnh c\\xF3 1 l\\u01B0u \\xFD l\\xE0 Catalyst Optimizer ch\\u1EC9 c\\xF3 DataFrame ho\\u1EB7c DataSet ho\\u1EB7c SQL query m\\u1EDBi c\\xF3 th\\u1EC3 ch\\u1EA1y qua 1 s\\u1ED1 Spark Feature nh\\u01B0 Catalyst Optimizer ho\\u1EB7c Tungsten Optimizer. N\\u1EBFu c\\xE1c b\\u1EA1n s\\u1EED d\\u1EE5ng RDD \\u0111\\u1EC3 process c\\xE1c b\\u1EA1n ph\\u1EA3i t\\u1EF1 t\\u1ED1i \\u01B0u.\"}),`\n`,(0,e.jsxs)(n.p,{children:[\"Sau khi c\\xF3 Optimize Logical plan, \\u1EDF Physical planning Spark s\\u1EBD generate ra nhi\\u1EC1u physical plan. Cost model s\\u1EBD t\\xEDnh cost c\\u1EE7a t\\u1EEBng Physical plan sao cho t\\u1ED1i \\u01B0u nh\\u1EA5t v\\xE0 ch\\u1ECDn n\\xF3, Ngo\\xE0i ra \",(0,e.jsx)(n.strong,{children:\"Cost-base optimization\"}),\" s\\u1EBD ch\\u1ECDn c\\xE1ch join sao cho ph\\xF9 h\\u1EE3p nh\\u1EA5t v\\u1EDBi data.\"]}),`\n`,(0,e.jsx)(n.h3,{children:\"Physical plan\"}),`\n`,(0,e.jsx)(n.p,{children:(0,e.jsx)(n.img,{src:\"/media/2022/07/spark-behind-9.png\",alt:\"\"})}),`\n`,(0,e.jsx)(n.p,{children:\"\\u1EDE physcal plan s\\u1EBD c\\xF3 2 b\\u01B0\\u1EDBc:\"}),`\n`,(0,e.jsx)(n.p,{children:\"B\\u01B0\\u1EDBc 1:\"}),`\n`,(0,e.jsx)(n.p,{children:\"T\\u1EA1o ra nh\\u1EEFng step s\\u1EED d\\u1EE5ng c\\xE1c strategies \\u1EE9ng v\\u1EDBi m\\u1ED7i node c\\u1EE7a logical plan, v\\xED d\\u1EE5:\"}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsx)(n.li,{children:\"Trong logical plan: JOIN\"}),`\n`,(0,e.jsx)(n.li,{children:\"Trong Physical plan: SortMergeJoin, BroadcastHashJoin\"}),`\n`]}),`\n`,(0,e.jsx)(n.p,{children:\"B\\u01B0\\u1EDBc 2:\"}),`\n`,(0,e.jsx)(n.p,{children:\"Final version plan s\\u1EBD \\u0111\\u01B0\\u1EE3c th\\u1EF1c hi\\u1EC7n, t\\u1EA1o ra RDD code\"}),`\n`,(0,e.jsx)(n.h4,{children:\"Operator\"}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsx)(n.li,{children:\"FileScan: mi\\xEAu t\\u1EA3 vi\\u1EC7c \\u0111\\u1ECDc data t\\u1EEB 1 format.\"}),`\n`,(0,e.jsx)(n.li,{children:\"Exchange: mi\\xEAu t\\u1EA3 vi\\u1EC7c shuffle - physical data movement tr\\xEAn cluster.\"}),`\n`,(0,e.jsx)(n.li,{children:\"HashAggregate, SortAggregate, ObjectHashAggregate: Mi\\xEAu t\\u1EA3 data aggregation.\"}),`\n`,(0,e.jsx)(n.li,{children:\"SortMergeJoin: Mi\\xEAu t\\u1EA3 vi\\u1EC7c join 2 dataframe, Exchange v\\xE0 sort th\\u01B0\\u1EDDng s\\u1EBD x\\u1EA3y ra tr\\u01B0\\u1EDBc khi SortMergeJoin nh\\u01B0ng kh\\xF4ng nh\\u1EA5t thi\\u1EBFt ph\\u1EA3i x\\u1EA3y ra.\"}),`\n`,(0,e.jsx)(n.li,{children:\"BroadcastHashJoin: Mi\\xEAu t\\u1EA3 vi\\u1EC7c join 2 dataframe.\"}),`\n`]}),`\n`,(0,e.jsx)(n.h4,{children:\"Additional Rules\"}),`\n`,(0,e.jsx)(n.p,{children:\"Ngo\\xE0i Operator c\\xF2n c\\xF3 nh\\u1EEFng rule nh\\u01B0:\"}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsx)(n.li,{children:\"EnsureRequirements\"}),`\n`,(0,e.jsx)(n.li,{children:\"ReuseExchange\"}),`\n`,(0,e.jsx)(n.li,{children:\"\\u2026\"}),`\n`]}),`\n`,(0,e.jsx)(n.p,{children:\"Sau khi ch\\u1ECDn ra \\u0111\\u01B0\\u1EE3c Physical plan ph\\xF9 h\\u1EE3p. Code generator s\\u1EBD generate Java code Binary v\\xE0 s\\u1EBD \\u0111\\u01B0\\u1EE3c th\\u1EF1 hi\\u1EC7n tr\\xEAn c\\xE1c worker.\"}),`\n`,(0,e.jsx)(n.h2,{children:\"Executor\"}),`\n`,(0,e.jsx)(n.p,{children:\"Spark app th\\u01B0\\u1EDDng s\\u1EBD start 1 ho\\u1EB7c nhi\\u1EC1u Executor \\u0111\\u1EC3 th\\u1EF1c hi\\u1EC7n task.\"}),`\n`,(0,e.jsx)(n.p,{children:\"M\\u1EB7c \\u0111\\u1ECBnh (Static Allocation of Executors) v\\u1EDBi ch\\u1EBF \\u0111\\u1ED9 n\\xE0y Executor th\\u01B0\\u1EDDng s\\u1EBD ch\\u1EA1y cho t\\u1EDBi khi n\\xE0o Spark app k\\u1EBFt th\\xFAc. Vi\\u1EC7c n\\xE0y d\\u1EABn \\u0111\\u1EBFn kh\\xF4ng t\\u1ED1i \\u01B0u v\\u1EC1 resource\"}),`\n`,(0,e.jsx)(n.p,{children:\"Kh\\xE1c v\\u1EDBi static (Dynamic Allocation). c\\xE1c Executor s\\u1EBD t\\u1EF1 \\u0111\\u1ED9ng remove khi th\\u1EF1c hi\\u1EC7n xong task. Vi\\u1EC7c n\\xE0y s\\u1EBD ti\\u1EBFt ki\\u1EC7m resource cho cluster.\"}),`\n`,(0,e.jsx)(n.p,{children:\"Ngo\\xE0i ra Executor report hearbeat v\\xE0 c\\xE1c metrics c\\u1EE7a task v\\u1EC1 cho driver.\"}),`\n`,(0,e.jsx)(n.p,{children:\"Executor c\\xF3 th\\u1EC3 run multiple task song song v\\xE0 tu\\u1EA7n t\\u1EF1, v\\xE0 tracking nh\\u1EEFng task \\u0111ang ch\\u1EA1y.\"}),`\n`,(0,e.jsx)(n.h1,{children:\"References\"}),`\n`,(0,e.jsx)(n.p,{children:(0,e.jsx)(n.a,{href:\"https://github.com/JerryLead/SparkInternals\",children:\"https://github.com/JerryLead/SparkInternals\"})}),`\n`,(0,e.jsx)(n.p,{children:(0,e.jsx)(n.a,{href:\"https://www.slideshare.net/databricks/physical-plans-in-spark-sql\",children:\"https://www.slideshare.net/databricks/physical-plans-in-spark-sql\"})}),`\n`,(0,e.jsx)(n.p,{children:(0,e.jsx)(n.a,{href:\"https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\",children:\"https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\"})}),`\n`,(0,e.jsx)(n.p,{children:(0,e.jsx)(n.a,{href:\"https://books.japila.pl/apache-spark-internals/\",children:\"https://books.japila.pl/apache-spark-internals/\"})}),`\n`,(0,e.jsx)(n.p,{children:\"Spark Submit Conference\"})]})}function f(i={}){let{wrapper:n}=i.components||{};return n?(0,e.jsx)(n,Object.assign({},i,{children:(0,e.jsx)(d,i)})):d(i)}var D=f;return S(x);})();\n;return Component;","toc":[],"frontMatter":{"readingTime":{"text":"11 min read","minutes":10.46,"time":627600,"words":2092},"slug":"apache-spark-behind-the-scienes","fileName":"2022-07-12-apache-spark-behind-the-scienes.md","title":"Apache Spark behind the scenes","authors":["hung"],"date":"2022-07-12T00:00:00.000Z","tags":["engineering","data"],"summary":"Các bạn có thắc mắc sau khi submit 1 job cho Spark Cluster thì Spark sẽ làm những gì không? Cùng tìm hiểu với mình nhé.","layout":"PostLayout"}},"authorDetails":[{"readingTime":{"text":"1 min read","minutes":0.175,"time":10500,"words":35},"slug":["hung"],"fileName":"hung.md","name":"Hung Tran","avatar":"https://avatars.githubusercontent.com/u/61528065?v=4","occupation":"Data Engineer","company":"Fossil Vietnam","email":"tghung@fossil.com","linkedin":"https://www.linkedin.com/in/hungtran97","github":"https://github.com/hungtran150","date":null}],"prev":{"title":"Working From Home","authors":["phuong"],"date":"2022-07-11T00:00:00.000Z","tags":["management"],"summary":"Working From Home hiện đang là working style trending, nhưng liệu có hiệu quả?","layout":"PostLayout","slug":"working-from-home"},"next":null},"__N_SSG":true}